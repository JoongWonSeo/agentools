{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AgenTools\n",
    "\n",
    "## Why should I exist?\n",
    "AgenTools aims to be what PyTorch Lightning is to PyTorch, but for LLM-based **assistants**, **copilots** and **agents**. Similar to how the training loop in deep learning essentially stays the same, AgenTools handles the boilerplate code and give you a both high-level and highly customizable interface.\n",
    "\n",
    "On the other hand, AgenTools is explicitly **not trying to be another LangChain, LlamaIndex or Haystack**. We want to abstract only the boilerplate away, but data connections, long pipelines, many API connections, etc. are not the focus of AgenTools, in order to be as lightweight as possible.\n",
    "\n",
    "\n",
    "## What can I do?\n",
    "There are, broadly speaking, two types of customization you might want:\n",
    "- **Assistant Implementation:** How each step of the \"assistant loop\" is implemented, and\n",
    "- **Assistant Integration:** How your application then reacts to what the assistant does.\n",
    "\n",
    "Often enough, the provided **assistant implementations** will be sufficient, and building an app around the assistant will be the main task. Therefore, let's first look at the latter:\n",
    "\n",
    "### Assistant Integration\n",
    "\n",
    "To allow maximum efficiency and flexibility, AgenTools centers around the concept of asynchronous **event generators**, which `yield` events that you can freely handle in your application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, what time is it? -> gpt-3.5-turbo\n",
      "[Assistant]: Calling get_date_time({})...\n",
      "[get_date_time(...)]: 2024-06-03 18:11:12\n",
      "[Assistant]: The current time is 18:11:12 on June 3, 2024.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from agentools import *\n",
    "\n",
    "\n",
    "@function_tool\n",
    "async def get_date_time():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "ass = Assistant(tools=get_date_time)\n",
    "\n",
    "async for event in ass.response_events(\"Hey, what time is it?\"):\n",
    "    match event:\n",
    "        # ========== Normal Events ========== #\n",
    "        case ResponseStartEvent():\n",
    "            print(f\"{event.prompt} -> {event.model}\")\n",
    "\n",
    "        case TextMessageEvent():\n",
    "            print(f\"[Assistant]: {event.content}\")\n",
    "\n",
    "        case ToolCallsEvent():\n",
    "            for call in event.tool_calls:\n",
    "                f = call.function\n",
    "                print(f\"[Assistant]: Calling {f.name}({f.arguments})...\")\n",
    "\n",
    "        case ToolResultEvent():\n",
    "            print(f\"[{event.tool_call.function.name}(...)]: {event.result}\")\n",
    "\n",
    "        # ========== Error Events ========== #\n",
    "        case MaxTokensExceededEvent():\n",
    "            print(\"[ERROR] Max token reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the above `print`s with a frontend connection, and you have a simple chatbot. Use `yield`s to turn it into a generator inside a FastAPI endpoint, and your endpoint supports streaming response to the client. `match` any events that you need to listen to.\n",
    "\n",
    "This is as concise and flexible as it gets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant Implementation\n",
    "\n",
    "While a simple LLM is as simple as *Prompt In -> Completion Out*, assistants with tools, and streaming support is much more complex to implement.\n",
    "\n",
    "The following pseudocode shows the loop that every assistant follows, for a single prompt with tools support:\n",
    "\n",
    "```python\n",
    "def assistant_loop(history, prompt, tools):\n",
    "    # add the prompt to the conversation history\n",
    "    history.append(user=prompt)\n",
    "\n",
    "    while True:\n",
    "        # generate the completion\n",
    "        completion = generate_completion(messages)\n",
    "\n",
    "        # select the response message, if there are multiple choices\n",
    "        response = completion.choices[0]\n",
    "        history.append(assistant=message)\n",
    "\n",
    "        # do something with the assistant's response\n",
    "        handle_text_content(response.content)\n",
    "\n",
    "        # (zero, one or more) tools called by the assistant\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_output = call_tool(tool_call, tools)\n",
    "            history.append(tool=tool_output)\n",
    "        \n",
    "        # if no tool was called, the response is final\n",
    "        if not response.tool_calls:\n",
    "            break\n",
    "```\n",
    "\n",
    "This loop gets much bigger if streaming must also be handled. Here's what `generate_completion` from above could look like:\n",
    "\n",
    "```python\n",
    "def generate_completion(messages):\n",
    "    # call the underlying LLM, e.g. through an API\n",
    "    completion_stream = call_llm_api(messages)\n",
    "\n",
    "    partial_completion = ... # gather the tokens here\n",
    "    \n",
    "    # the stream yields new tokens as they are generated by the LLM\n",
    "    for delta_chunk in completion_stream:\n",
    "        # accumulate the new chunks of tokens\n",
    "        partial_completion.extend(delta_chunk)\n",
    "        \n",
    "        # select the response message, if there are multiple choices\n",
    "        partial_response = partial_completion.choices[0]\n",
    "\n",
    "        # do something with the assistant's partial response\n",
    "        handle_partial_text_content(partial_response.content)\n",
    "\n",
    "        # do something with the assistant's partial tool calls\n",
    "        handle_partial_tool_calls(partial_response.tool_calls)\n",
    "    \n",
    "    # all tokens have been generated and gathered\n",
    "    return partial_completion\n",
    "```\n",
    "\n",
    "And again, the `handle_partial_tool_calls` function could be quite complex, if you want to do something fancy with them in the frontend, such as displaying a preview of the tool's output from the partial completion.  \n",
    "AgenTools also offers additional utilities to ease handling partial tool calls, e.g. a custom JSON parser that can deal with incomplete JSON objects, or preview functions that you can define for each tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentools-Ziq7AQeI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
